# CviÄenÃ­ 1: AutomatickÃ© doplÅˆovÃ¡nÃ­ slov (N-gram model)

V tÃ©to Ãºloze byste si mÄ›li vyzkouÅ¡et vytvoÅ™it jednoduchÃ½ model jazyka. Model jazyka je statistickÃ½m modelem, kterÃ½ se
snaÅ¾Ã­ odhadnout pravdÄ›podobnost vÃ½skytu slova na zÃ¡kladÄ› jeho kontextu. V tÃ©to Ãºloze se zamÄ›Å™Ã­me na n-gram model, kterÃ½
je zaloÅ¾en na pravdÄ›podobnosti vÃ½skytu n po sobÄ› jdoucÃ­ch slov. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci
v libovolnÃ©m rozsahu.

## ZÃ¡kladnÃ­ seznÃ¡menÃ­ s n-gramy (jednoduÅ¡Å¡Ã­) - 1 bod

### Ãškol:

- NaÄtÄ›te zvolenÃ½ ÄeskÃ½ textovÃ½ soubor napÅ™. [opus.nlpl.eu](https://opus.nlpl.eu/results/en&cs/corpus-result-table).
- RozdÄ›lte text na tokeny (slova). MÅ¯Å¾ete si vybrat, zda odstranÃ­te interpunkci (pokud ÄÃ¡rky a teÄky v textu zachovÃ¡te,
  pak je berte jako jedno slovo) a zda pÅ™evedete text na malÃ¡ pÃ­smena.
- VytvoÅ™te unigramovÃ½ (jednoslovnÃ½), bigramovÃ½ (dvoslovnÃ½) a trigramovÃ½ (tÅ™Ã­slovnÃ½) model frekvencÃ­ slov.
- Zobrazte nejÄastÄ›jÅ¡Ã­ n-gramy.

### CÃ­l:

NauÄit se zÃ¡kladnÃ­ prÃ¡ci s textem a pochopit, co jsou n-gramy.

## PravdÄ›podobnost n-gramÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Na zÃ¡kladÄ› VaÅ¡ich dat vytvoÅ™te bigramovÃ½ a trigramovÃ½ model pÅ™epoÄÃ­tanÃ½ na pravdÄ›podobnosti.
- VypoÄÃ­tejte pravdÄ›podobnost vÃ½skytu slova nÃ¡sledujÃ­cÃ­ho po urÄitÃ©m n-gramu.
- ZjistÄ›te, co to je Laplaceovo vyhlazovÃ¡nÃ­ a upravte vÃ½poÄet pravdÄ›podobnosti.

### CÃ­l:

SeznÃ¡mit se s pravdÄ›podobnostnÃ­m modelem n-gramÅ¯ a zÃ¡kladnÃ­m vyhlazovÃ¡nÃ­m.

## Predikce slova pomocÃ­ bigramovÃ©ho modelu (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Implementujte jednoduchÃ½ autokomplet, kterÃ½ na zÃ¡kladÄ› zadanÃ©ho slova nabÃ­dne nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo.
- Testujte na rÅ¯znÃ½ch vstupech a porovnejte vÃ½sledky.

### CÃ­l:

Pochopit princip predikce slov a bÃ½t schopen vytvoÅ™it zÃ¡kladnÃ­ model automatickÃ©ho doplÅˆovÃ¡nÃ­.

## VytvoÅ™enÃ­ generÃ¡toru textu (obtÃ­Å¾nÄ›jÅ¡Ã­) - 2 body

### Ãškol:

- PomocÃ­ trigramovÃ©ho modelu vytvoÅ™te jednoduchÃ½ generÃ¡tor textu.
- Model by mÄ›l na zÃ¡kladÄ› vstupnÃ­ho slova vygenerovat dalÅ¡Ã­ slova a vytvoÅ™it nÄ›kolik vÄ›t.

### CÃ­l:

Aplikovat n-gram model pro tvorbu textu.

## Evaluace modelu pomocÃ­ perplexity (nÃ¡roÄnÄ›jÅ¡Ã­) - 2 body

## Ãškol:

- Nastudujte si pojem perplexity.
- Implementujte metodu pro vÃ½poÄet perplexity trÃ©novanÃ©ho n-gram modelu.
- Porovnejte perplexity rÅ¯znÃ½ch n-gram modelÅ¯ (unigramy, bigramy, trigramy).
- ZhodnoÅ¥te kvalitu modelu na testovacÃ­m datasetu.

### CÃ­l:

NauÄit se mÄ›Å™it kvalitu jazykovÃ©ho modelu a pochopit vÃ½znam perplexity.

## KontrolnÃ­ otÃ¡zky k Ãºloze: AutomatickÃ© doplÅˆovÃ¡nÃ­ slov (N-gram model)

- ZÃ¡kladnÃ­ seznÃ¡menÃ­ s n-gramy
    - Co je to n-gram?
    - Jak se poÄÃ­tajÃ­ frekvence unigramÅ¯, bigramÅ¯ a trigramÅ¯?
- PravdÄ›podobnost n-gramÅ¯
    - Jak se poÄÃ­tÃ¡ pravdÄ›podobnost vÃ½skytu slova v rÃ¡mci n-gramovÃ©ho modelu?
    - Co je Laplaceovo vyhlazovÃ¡nÃ­ a proÄ se pouÅ¾Ã­vÃ¡?
- Predikce slova pomocÃ­ bigramovÃ©ho modelu
    - Jak lze pouÅ¾Ã­t bigramovÃ½ model pro automatickÃ© doplÅˆovÃ¡nÃ­ slov?
    - Jak se urÄuje nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo?
- VytvoÅ™enÃ­ generÃ¡toru textu
    - Jak lze trigramovÃ½ model vyuÅ¾Ã­t pro generovÃ¡nÃ­ textu?
    - Jak lze vylepÅ¡it kvalitu generovanÃ©ho textu?
- Evaluace modelu pomocÃ­ perplexity
    - Co je to perplexity a jak se vypoÄÃ­tÃ¡vÃ¡?
    - JakÃ¡ je interpretace hodnoty perplexity pro jazykovÃ½ model?
    - Jak se perplexity mÄ›nÃ­ s rostoucÃ­ velikostÃ­ n-gramu?

# CviÄenÃ­ 2: PÅ™Ã­mÃ© vyhledÃ¡vÃ¡nÃ­ v textovÃ½ch datech - 7 bodÅ¯

V tÃ©to Ãºloze budete analyzovat rÅ¯znÃ© algoritmy pro vyhledÃ¡vÃ¡nÃ­ vzorÅ¯ v textu. ZamÄ›Å™Ã­te se na porovnÃ¡nÃ­ tÅ™Ã­ algoritmÅ¯:
hrubÃ© sÃ­ly, Knuth-Morris-Pratt (KMP) a Boyer-Moore-Horspool (BMH). CÃ­lem je pochopit, kdy je kterÃ½ algoritmus vÃ½hodnÄ›jÅ¡Ã­
a jak se chovajÃ­ pÅ™i rÅ¯znÃ½ch typech textÅ¯ a vzorÅ¯. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m
rozsahu.

## PÅ™Ã­prava implementacÃ­ (jednoduÅ¡Å¡Ã­) - 2 body

### Ãškol:

- PÅ™ipravte implementace tÅ™Ã­ algoritmÅ¯: hrubÃ¡ sÃ­la, KMP a BMH (mÅ¯Å¾ete vyuÅ¾Ã­t AI).
- Upravte algoritmy tak, aby vracely nejen nalezenÃ© pozice vzoru, ale i poÄet porovnÃ¡nÃ­ znakÅ¯.

### CÃ­l:

ZÃ­skat implementace algoritmÅ¯ a zajistit, aby poskytovaly statistiky o porovnÃ¡nÃ­ znakÅ¯.

## TestovÃ¡nÃ­ na rÅ¯znÃ½ch datech (stÅ™ednÃ­) - 2 body

### Ãškol:

Otestujte implementace na tÅ™ech rÅ¯znÃ½ch typech textÅ¯:

- KrÃ¡tkÃ½ text (~100 znakÅ¯)
- DlouhÃ½ text (~1000 znakÅ¯)
- NÃ¡hodnÄ› generovanÃ½ text z malÃ© abecedy (cca do ÄtyÅ™ znakÅ¯, napÅ™. sekvence DNA â€AGCTAGCTâ€¦â€œ)

Pro kaÅ¾dÃ½ typ textu proveÄte testy s alespoÅˆ tÅ™emi rÅ¯znÃ½mi vzory.

### CÃ­l:

OvÄ›Å™it, jak algoritmy fungujÃ­ na rÅ¯znÃ½ch datech.

## PorovnÃ¡nÃ­ poÄtu porovnÃ¡nÃ­ znakÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Zaznamenejte poÄet porovnÃ¡nÃ­ znakÅ¯ pro kaÅ¾dÃ½ algoritmus a kaÅ¾dou testovacÃ­ sadu.
- VytvoÅ™te tabulku s vÃ½sledky.

### CÃ­l:

Kvantifikovat efektivitu algoritmÅ¯.

## Vizualizace vÃ½konu algoritmÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- VytvoÅ™te graf, kterÃ½ ukÃ¡Å¾e efektivitu algoritmÅ¯ v zÃ¡vislosti na dÃ©lce textu a vzoru.

### CÃ­l:

Graficky zobrazit vÃ½konnost algoritmÅ¯.

## AnalÃ½za a rozhodovÃ¡nÃ­ o vhodnosti algoritmÅ¯ (nÃ¡roÄnÄ›jÅ¡Ã­) - 1 bod

### Ãškol:

OdpovÄ›zte na nÃ¡sledujÃ­cÃ­ otÃ¡zky:

- Kdy se KMP chovÃ¡ lÃ©pe neÅ¾ BMH?
- Kdy je BMH rychlejÅ¡Ã­ neÅ¾ Brute Force?
- Kdy je KMP nevÃ½hodnÃ© pouÅ¾Ã­vat?
- Jak algoritmy fungujÃ­ na textech s opakujÃ­cÃ­mi se vzory?

### CÃ­l:

Pochopit silnÃ© a slabÃ© strÃ¡nky jednotlivÃ½ch algoritmÅ¯.

## ğŸ¯ BonusovÃ¡ Ãºloha (+2 body navÃ­c)

### Ãškol:

NavrhnÄ›te hybridnÃ­ pÅ™Ã­stup:

- VytvoÅ™te heuristiku, kterÃ¡ na zÃ¡kladÄ› dÃ©lky a vlastnostÃ­ vzoru a textu vybere nejvhodnÄ›jÅ¡Ã­ algoritmus.
- Porovnejte vÃ½konnost tÃ©to strategie oproti jednotlivÃ½m algoritmÅ¯m.

# CviÄenÃ­ 3: AutomatickÃ¡ oprava slov a vyhledÃ¡vÃ¡nÃ­ s chybou - 7 bodÅ¯

V tÃ©to Ãºloze budete implementovat algoritmus pro automatickou opravu slov a analyzovat efektivitu rÅ¯znÃ½ch pÅ™Ã­stupÅ¯ k
vyhledÃ¡vÃ¡nÃ­ slov s chybou. ZÃ¡kladnÃ­ inspiracÃ­ pro implementaci je znÃ¡mÃ½ algoritmus Petera Norwiga. VaÅ¡Ã­m cÃ­lem bude
implementovat vÃ½poÄet editaÄnÃ­ vzdÃ¡lenosti a nÃ¡slednÄ› vytvoÅ™it systÃ©m pro automatickou opravu slov na zÃ¡kladÄ›
pravdÄ›podobnosti vÃ½skytu slov ve slovnÃ­ku. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m
rozsahu.

## VÃ½poÄet editaÄnÃ­ vzdÃ¡lenosti (jednoduÅ¡Å¡Ã­) - 3 body

### Ãškol:

- Implementujte algoritmus pro vÃ½poÄet Levenshteinovy vzdÃ¡lenosti mezi dvÄ›ma slovy.
- Otestujte vaÅ¡i implementaci na vlastnÃ­ch datech â€“ vyberte nÄ›kolik dvojic slov a ovÄ›Å™te, zda vzdÃ¡lenost odpovÃ­dÃ¡
  oÄekÃ¡vÃ¡nÃ­.

### CÃ­l:

PorozumÄ›t principu editaÄnÃ­ vzdÃ¡lenosti a zajistit sprÃ¡vnou implementaci.

## Implementace automatickÃ© opravy slov (stÅ™ednÃ­) - 4 body

### PÅ™Ã­prava slovnÃ­ku - 1 bod

- VytvoÅ™te slovnÃ­k slov na zÃ¡kladÄ› zvolenÃ©ho datasetu.
- UloÅ¾te frekvenci jednotlivÃ½ch slov, aby bylo moÅ¾nÃ© urÄit jejich pravdÄ›podobnost vÃ½skytu.

### GenerovÃ¡nÃ­ variant slov - 1 bod

- Pro vstupnÃ­ slovo vygenerujte vÅ¡echny moÅ¾nÃ© varianty slov s editaÄnÃ­ vzdÃ¡lenostÃ­ maximÃ¡lnÄ› 2.
- UvaÅ¾ujte operace vloÅ¾enÃ­, smazÃ¡nÃ­, nahrazenÃ­, prohozenÃ­ sousedÅ¯.
- ZjistÄ›te poÄet variant.

### VÃ½bÄ›r nejpravdÄ›podobnÄ›jÅ¡Ã­ho slova - 1 bod

- Z vygenerovanÃ½ch variant vyberte nejpravdÄ›podobnÄ›jÅ¡Ã­ slovo podle jeho Äetnosti ve slovnÃ­ku.
- Opravte nÃ¡sledujÃ­cÃ­ vÄ›tu:  
  _Dneska si dÃ¡m obÄ›Å¥ v restauarci a pak pÅ¯jdu zpÄ›Å¥ domÅ¯, kde se podÃ­vÃ¡m na televezÃ­._

### AlternativnÃ­ pÅ™Ã­stup a porovnÃ¡nÃ­ efektivity - 1 bod

- MÃ­sto generovÃ¡nÃ­ variant vypoÄÃ­tejte editaÄnÃ­ vzdÃ¡lenost ke vÅ¡em slovÅ¯m ve slovnÃ­ku a vyberte nejbliÅ¾Å¡Ã­ kandidÃ¡ty.
- Porovnejte vÃ½poÄetnÃ­ sloÅ¾itost a kvalitu vÃ½sledkÅ¯ obou pÅ™Ã­stupÅ¯.
- Pro dÃ©lku _n_ nÄ›jakÃ©ho slova urÄete poÄet vygenerovanÃ½ch variant.

## ğŸ¯ BonusovÃ¡ Ãºloha (+2 body navÃ­c)

- VylepÅ¡enÃ­ vÃ½poÄtu pravdÄ›podobnosti pomocÃ­ n-gram modelu:
    - NavrhnÄ›te a implementujte vylepÅ¡enÃ½ systÃ©m, kterÃ½ vyuÅ¾Ã­vÃ¡ n-gramy a podmÃ­nÄ›nÃ© pravdÄ›podobnosti pro urÄenÃ­
      nejpravdÄ›podobnÄ›jÅ¡Ã­ho opravenÃ©ho slova.
    - Porovnejte vÃ½sledky s pÅ¯vodnÃ­m pÅ™Ã­stupem a analyzujte zlepÅ¡enÃ­.

# CviÄenÃ­ 4: Boolean Information Retrieval â€“ InvertovanÃ½ index a dotazy â€“ 7 bodÅ¯

V tÃ©to Ãºloze si vyzkouÅ¡Ã­te zÃ¡kladnÃ­ principy booleovskÃ©ho vyhledÃ¡vÃ¡nÃ­ v textovÃ½ch datech. VytvoÅ™Ã­te invertovanÃ½ index s
normalizacÃ­ tokenÅ¯, naparsujete a vyhodnotÃ­te dotazy se zÃ¡vorkami a rÅ¯znÃ½mi logickÃ½mi operÃ¡tory. NÃ¡slednÄ› rozÅ¡Ã­Å™Ã­te svÅ¯j
systÃ©m o kompaktnÃ­ reprezentaci indexu a analyzujete jeho efektivitu. Ãšloha je urÄena pro hlubÅ¡Ã­ pochopenÃ­ principÅ¯
klasickÃ©ho IR modelu. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m rozsahu.

### Ke studiu

- [Introduction to Information
  Retrieval â€“ Chapter 1, pages 7-9.](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)

## Ãškoly

### InvertovanÃ½ index s normalizacÃ­ â€“ 1 bod

**Ãškol:**

- VytvoÅ™te textovÃ½ korpus (alespoÅˆ 50 dokumentÅ¯) a vytvoÅ™te invertovanÃ½ index.
- PÅ™i tvorbÄ› indexu odstraÅˆte stop slova.
- Index by mÄ›l uchovÃ¡vat takÃ© Äetnost vÃ½skytu kaÅ¾dÃ©ho slova v jednotlivÃ½ch dokumentech.

**CÃ­l:**  
SeznÃ¡mit se s reprezentacÃ­ textovÃ©ho korpusu pomocÃ­ invertovanÃ©ho indexu a pÅ™edzpracovÃ¡nÃ­m textu.

### ParsovÃ¡nÃ­ a vyhodnocenÃ­ sloÅ¾itÃ½ch boolean dotazÅ¯ â€“ 2 body

**Ãškol:**

- Implementujte zpracovÃ¡nÃ­ boolean dotazÅ¯ vÄetnÄ› operÃ¡torÅ¯ `AND`, `OR`, `NOT`.
- VyhodnocenÃ­ proveÄte jako mnoÅ¾inovÃ© operace nad invertovanÃ½m indexem.

**CÃ­l:**  
NauÄit se sprÃ¡vnÄ› parsovat sloÅ¾itÄ›jÅ¡Ã­ logickÃ© dotazy a efektivnÄ› je vyhodnocovat.

### Efektivita a velikost indexu â€“ 1 bod

**Ãškol:**

- Analyzujte velikost vytvoÅ™enÃ©ho indexu: kolik obsahuje tokenÅ¯, prÅ¯mÄ›rnÃ¡ dÃ©lka seznamÅ¯, celkovÃ½ poÄet zÃ¡znamÅ¯.

**CÃ­l:**  
UvÄ›domit si nÃ¡roky invertovanÃ©ho indexu.

### RozhranÃ­ pro dotazovÃ¡nÃ­ a srovnÃ¡nÃ­ dotazÅ¯ â€“ 1 bod

**Ãškol:**

- VytvoÅ™te jednoduchÃ© rozhranÃ­ (napÅ™. konzolovÃ© nebo skriptovÃ©), kterÃ© umoÅ¾nÃ­ zadÃ¡nÃ­ libovolnÃ©ho dotazu a zobrazenÃ­
  vÃ½sledkÅ¯.
- Zobrazte alespoÅˆ ID dokumentÅ¯ nebo prvnÃ­ vÄ›tu z kaÅ¾dÃ©ho nalezenÃ©ho dokumentu.

**CÃ­l:**  
Usnadnit prÃ¡ci s vyhledÃ¡vaÄem a demonstrovat dopad rÅ¯znÃ½ch dotazÅ¯ na vÃ½stup.

### RozÅ¡Ã­Å™enÃ½ boolean model s vÃ¡hovÃ¡nÃ­m â€“ 2 body

**Ãškol:**

- NavrhnÄ›te a implementujte jednoduchÃ½ rozÅ¡Ã­Å™enÃ½ boolean model, kterÃ½ umoÅ¾Åˆuje pÅ™iÅ™azenÃ­ skÃ³re dokumentÅ¯m.
- Na vstupu je dotaz, na vÃ½stupu seÅ™azenÃ½ seznam dokumentÅ¯ podle skÃ³re relevance.
- Porovnejte kvalitu vÃ½sledkÅ¯ oproti ÄistÃ©mu boolean pÅ™Ã­stupu.

**CÃ­l:**  
SeznÃ¡mit se s principem vÃ¡Å¾enÃ©ho boolean modelu a Ãºvodem do relevance ranking.

# CviÄenÃ­ 5: VektorovÃ½ model a vÃ½poÄet tf-idf â€“ 7 bodÅ¯

V tomto cviÄenÃ­ si vyzkouÅ¡Ã­te praktickou prÃ¡ci s vektorovÃ½m modelem reprezentace dokumentÅ¯. RuÄnÄ› spoÄÃ­tÃ¡te tf-idf vÃ¡hy,
porovnÃ¡te dokumenty pomocÃ­ kosinovÃ© podobnosti a zamyslÃ­te se nad limity tÃ©to metody. Ãšloha je navrÅ¾ena tak, abyste
porozumÄ›li principÅ¯m vÃ¡Å¾enÃ­ slov a podobnosti dokumentÅ¯, nikoli jen pouÅ¾ili hotovÃ© funkce. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete
pouÅ¾Ã­vat umÄ›lou inteligenci pro implementaci i konzultaci nÃ¡vrhu, ale vÃ½stupy musÃ­ bÃ½t vaÅ¡Ã­m vlastnÃ­m zpracovÃ¡nÃ­m a
interpretacÃ­ a oÄekÃ¡vÃ¡ se vaÅ¡e schopnost problematiku vysvÄ›tlit, nikoli pouze pÅ™edloÅ¾it vÃ½stup nÃ¡stroje.

## PÅ™edzpracovÃ¡nÃ­ textu â€“ 1 bod

### Ãškol:

- Vyberte si dataset s alespoÅˆ 20 dokumenty, mÅ¯Å¾ete pouÅ¾Ã­t napÅ™. [NLTK](https://www.nltk.org/nltk_data/), Gutenberg,
  Twitter, recenze atd.
- Pro zadanÃ© dokumenty proveÄte:
    - pÅ™evod na malÃ¡ pÃ­smena,
    - odstranÄ›nÃ­ interpunkce,
    - tokenizaci,
    - odstranÄ›nÃ­ stopslov.
- VytvoÅ™te si vlastnÃ­ seznam stopslov (alespoÅˆ 5 vÃ½razÅ¯).
- VÃ½stupem by mÄ›l bÃ½t seznam termÅ¯ pro kaÅ¾dÃ½ dokument.

**CÃ­l:** SeznÃ¡mit se s manuÃ¡lnÃ­m pÅ™edzpracovÃ¡nÃ­m textu a pÅ™ipravit jej pro vektorovou reprezentaci.

## VÃ½poÄet tf a idf â€“ 2 body

### Ãškol:

- SpoÄÃ­tejte term frequency (tf) kaÅ¾dÃ©ho slova *t* ve vÅ¡ech dokumentech.
    - PouÅ¾ijte nÄ›jakou formu normovÃ¡nÃ­ Äetnosti (relativnÃ­ Äetnost) nebo zdÅ¯vodnÄ›te pouÅ¾itÃ­ nenormovanÃ© verze.
- SpoÄÃ­tejte inverse document frequency (idf) s vyuÅ¾itÃ­m vzorce:

  $$  idf(t) = \log \left(\frac{N}{df(t)}\right)  $$

- SpoÄÃ­tejte tf-idf vÃ¡hy:

  $$  tf\text{-}idf(t,d) = tf(t,d) \times idf(t)  $$

- SpoÄÃ­tejte skÃ³re pro termy v dotazu *q*:

  $$  Score(q,d) = \sum_{t \in q} tf\text{-}idf(t,d)  $$

- VraÅ¥te dokumenty setÅ™Ã­dÄ›nÃ© podle skÃ³re.

**CÃ­l:** PorozumÄ›t vÃ½poÄtu jednotlivÃ½ch komponent tf-idf a jejich vÃ½znamu v kontextu textovÃ©ho korpusu.

## VÃ½poÄet tf-idf a kosinovÃ¡ podobnost â€“ 2 body

### Ãškol:

- SpoÄÃ­tejte kosinovou podobnost mezi vÅ¡emi dvojicemi dokumentÅ¯.
- UrÄete, kterÃ© dva dokumenty jsou si nejpodobnÄ›jÅ¡Ã­, a interpretujte proÄ.
- Jak by se vÃ½sledky zmÄ›nily, kdyby se pouÅ¾ilo jen tf bez idf?

**CÃ­l:** Prakticky aplikovat vektorovÃ½ model a pochopit princip vÃ½poÄtu podobnosti dokumentÅ¯.

## VÃ½znam idf v rÅ¯znÃ½ch domÃ©nÃ¡ch â€“ 1 bod (Ãºvaha)

### Ãškol:

- UveÄte pÅ™Ã­klad oblasti nebo tÃ©matu, kde by ÄastÃ¡ slova mohla bÃ½t navzdory vysokÃ© frekvenci velmi dÅ¯leÅ¾itÃ¡.
- VysvÄ›tlete, proÄ v takovÃ©m pÅ™Ã­padÄ› mÅ¯Å¾e bÃ½t pouÅ¾itÃ­ klasickÃ©ho idf nevhodnÃ©.
- NavrhnÄ›te Ãºpravu vÃ½poÄtu, kterÃ¡ by tento problÃ©m zmÃ­rnila.

**CÃ­l:** Kriticky zhodnotit omezenÃ­ vektorovÃ©ho modelu a navrhnout jeho Ãºpravy pro konkrÃ©tnÃ­ situace.

## NÃ¡vrh alternativnÃ­ho vÃ¡hovacÃ­ho schÃ©matu â€“ 1 bod (Ãºvaha)

### Ãškol:

- NavrhnÄ›te vÃ¡hovacÃ­ schÃ©ma pro krÃ¡tkÃ© texty (napÅ™. tweety), kterÃ© by lÃ©pe zachytilo vÃ½znam slov neÅ¾ klasickÃ© tf-idf.
- PopiÅ¡te, jak by vaÅ¡e schÃ©ma vÃ¡Å¾ilo slova:
    - velmi ÄastÃ¡ napÅ™Ã­Ä korpusem,
    - vyskytujÃ­cÃ­ se pouze jednou,
    - vyskytujÃ­cÃ­ se v ÄÃ¡sti dokumentÅ¯.

**CÃ­l:** PodpoÅ™it kreativnÃ­ pÅ™Ã­stup k nÃ¡vrhu vlastnÃ­ch modelÅ¯ a pochopenÃ­ vÃ½znamu jednotlivÃ½ch komponent vÃ¡Å¾enÃ­.

## CviÄenÃ­ 6: Komprese invertovanÃ©ho indexu â€“ 5 bodÅ¯

(Tohle se nepovedlo, nedostal jsem body za druhou polovinu, tak na to tu upozorÅˆuji, nepochopil jsem to, Å¡patnÄ› jsem si
to obhÃ¡jil a Å¡el jsem na to ÃºplnÄ› Å¡patnÄ›, implementaci kompresnÃ­ch algoritmÅ¯ jsem udÄ›lal sprÃ¡vnÄ›, ale pak jsem to Å¡patnÄ›
otestoval, mÃ­sto genereovÃ¡nÃ­ ASCII znakÅ¯ jsem mÄ›l generovat ÄÃ­sla, coÅ¾ mi dÃ¡vÃ¡ menÅ¡Ã­ smysl, ale budiÅ¾, v zdaÃ¡nÃ­ jsou
slova, ale nebudu se hÃ¡dat, tÃ­m pÃ¡dem ty velikosti a Äasy nedÃ¡vajÃ­ smysl, Äas jsem taky testoval na jednom dokumentu,
coÅ¾ je moje chyba, protoÅ¾e to takhle nenÃ­ statisticky sprÃ¡vnÄ›, mÄ›l jsem to testovat na vÃ­ce dokumentech a poÄÃ­tat
prÅ¯mÄ›r, tÃ­m Å¾e jsem to nepochopil a uÅ¾ jsem (ne)dostal body, tak to tu jen tak napÃ­Å¡u mÃ­stot toho, abych to opravoval)

V tomto cviÄenÃ­ si vyzkouÅ¡Ã­te rÅ¯znÃ© metody bezztrÃ¡tovÃ© komprese seznamu dokumentovÃ½ch identifikÃ¡torÅ¯ (docIDs) v
invertovanÃ©m indexu. ZamÄ›Å™Ã­te se na jejich implementaci, experimentÃ¡lnÃ­ vyhodnocenÃ­ kompresnÃ­ho pomÄ›ru i vlivu na
rychlost vyhledÃ¡vÃ¡nÃ­. Ãšloha vÃ¡s provede zÃ¡kladnÃ­mi technikami komprese pomocÃ­ kÃ³dovÃ¡nÃ­ rozdÃ­lÅ¯ a univerzÃ¡lnÃ­ch kÃ³dÅ¯.

### Implementace kompresnÃ­ch algoritmÅ¯ â€“ 3 body

**Ãškol:**

- Implementujte kompresi i dekompresi tÅ™Ã­ univerzÃ¡lnÃ­ch kÃ³dÅ¯:
    - **UnÃ¡rnÃ­ kÃ³dovÃ¡nÃ­/dekÃ³dovÃ¡nÃ­** (1 bod)  
      VysvÄ›tlete a naimplementujte.
    - **Eliasovo gamma kÃ³dovÃ¡nÃ­/dekÃ³dovÃ¡nÃ­** (1 bod)  
      VysvÄ›tlete a naimplementujte.
    - **Fibonacciho kÃ³dovÃ¡nÃ­/dekÃ³dovÃ¡nÃ­** (1 bod)  
      VysvÄ›tlete a naimplementujte.
- KÃ³dovÃ¡ slova reprezentujte v textovÃ© podobÄ›, nenÃ­ nutnÃ© je uklÃ¡dat binÃ¡rnÄ›.

**CÃ­l:**  
Pochopit princip univerzÃ¡lnÃ­ho kÃ³dovÃ¡nÃ­ a vytvoÅ™it funkÄnÃ­ implementaci pro experimenty.

### Simulace dat a kÃ³dovÃ¡nÃ­ â€“ 1 bod

**Ãškol:**

- Vygenerujte slovnÃ­k s 1000 nÃ¡hodnÃ½mi slovy a pÅ™edpoklÃ¡dejte kolekci s 10 000 dokumenty.
- VytvoÅ™te milion nÃ¡hodnÃ½ch unikÃ¡tnÃ­ch dvojic *(slovo, docID)* a sestavte invertovanÃ½ seznam docIDs pro kaÅ¾dÃ© slovo.
- Seznam docIDs pro kaÅ¾dÃ© slovo seÅ™aÄte a zakÃ³dujte jako sekvenci rozdÃ­lÅ¯ mezi po sobÄ› jdoucÃ­mi hodnotami, zvlÃ¡Å¡Å¥ pro
  kaÅ¾dÃ½ ze tÅ™Ã­ kÃ³dovacÃ­ch algoritmÅ¯.

**CÃ­l:**  
OvÄ›Å™it funkÄnost komprese na synteticky vytvoÅ™enÃ½ch datech a pÅ™ipravit podklady pro srovnÃ¡nÃ­ velikostÃ­.

### SrovnÃ¡nÃ­ velikostÃ­ a rychlosti â€“ 1 bod

**Ãškol:**

- Porovnejte velikost zakÃ³dovanÃ©ho seznamu se seznamem nezakÃ³dovanÃ½m (napÅ™. jako seznam ÄÃ­sel v textovÃ© podobÄ›).
- Otestujte vyhledÃ¡vÃ¡nÃ­ konkrÃ©tnÃ­ho docID ve vÅ¡ech variantÃ¡ch a urÄete rozdÃ­l v dobÄ› bÄ›hu (napÅ™. pomocÃ­ ÄasovÃ© funkce).

**CÃ­l:**  
KvantitativnÄ› zhodnotit pÅ™Ã­nos i cenu komprese z pohledu velikosti a vÃ½konu.

### DoporuÄenÃ© zdroje:

- [Prezentace ACS â€“ slide kÃ³dovÃ¡nÃ­ s promÄ›nlivÃ½m poÄtem bytÅ¯](https://homel.vsb.cz/~vas218/pdf/acs/lecture3-ext.pdf)
- [Introduction to Information Retrieval â€“ Kapitola 5, str. 96â€“98](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)
- [ÄŒlÃ¡nek: PorovnÃ¡nÃ­ rÅ¯znÃ½ch prefixovÃ½ch kÃ³dÅ¯](https://pdfs.semanticscholar.org/add5/81f36e848c47c4a1d7a0d1b72acc0ced7420.pdf)

# CviÄenÃ­ 7: PÅ™eklad slov pomocÃ­ vektorovÃ½ch reprezentacÃ­ â€“ 9 bodÅ¯

V tomto cviÄenÃ­ si vyzkouÅ¡Ã­te pÅ™enos vÃ½znamu mezi jazyky pomocÃ­ word embeddingÅ¯ a lineÃ¡rnÃ­ transformace. Nejprve zÃ­skÃ¡te
dvojjazyÄnÃ¡ data (vektorovÃ© reprezentace a pÅ™ekladovÃ© dvojice), nÃ¡slednÄ› implementujete metodu pro uÄenÃ­ transformaÄnÃ­
matice pomocÃ­ gradient descent a nakonec ovÄ›Å™Ã­te kvalitu pÅ™ekladu pomocÃ­ pÅ™esnosti. CviÄenÃ­ spojuje praktickÃ©
programovÃ¡nÃ­ s pochopenÃ­m matematickÃ©ho zÃ¡kladu.

## PÅ™Ã­prava dat â€“ 2 body

**VstupnÃ­ otÃ¡zka:** Co to je embedding?

**Ãškol:**

- StÃ¡hnÄ›te si dva embeddingy z webu [FastText](https://fasttext.cc/docs/en/crawl-vectors.html) (napÅ™. ÄeÅ¡tina a
  angliÄtina), v textovÃ©m formÃ¡tu `.vec`.
- StÃ¡hnÄ›te si pÅ™ekladovÃ© dvojice z
  projektu [MUSE â€“ bilingual dictionaries](https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries),
  konkrÃ©tnÄ› soubory `cs-en train` a   `cs-en test`.
- NaÄtÄ›te embeddingy a vytvoÅ™te matice **X** (zdrojovÃ½ jazyk) a **Y** (cÃ­lovÃ½ jazyk) pro trÃ©novacÃ­ a testovacÃ­ ÄÃ¡st.

**CÃ­l:** ZÃ­skat konzistentnÃ­ embeddingy a pÅ™ekladovÃ© pÃ¡ry pro experimenty bez nutnosti vytvÃ¡Å™et vlastnÃ­ slovnÃ­k.

## Implementace trÃ©novacÃ­ho algoritmu â€“ 5 bodÅ¯

**VstupnÃ­ otÃ¡zka:** Co dÄ›lÃ¡ gradient descent?

**Ãškol:**

- Implementujte funkci pro vÃ½poÄet Frobeniovy normy: $\|XW^\top - Y\|_F^2$ *(1 bod)*
- VypoÄÃ­tejte vÃ½raz $XW^\top - Y$ *(0.5 bodu)*
- OdvoÄte gradient ztrÃ¡tovÃ© funkce vÅ¯Äi matici $W^\top$ *(1 bod)*
- Implementujte gradient ztrÃ¡tovÃ© funkce vÅ¯Äi matici $W^\top$ *(1 bod)*
- Implementujte gradient descent s parametrem `alpha` (uÄÃ­cÃ­ koeficient) *(1 bod)*
- VytvoÅ™te uÄenÃ­ ve smyÄce: pevnÃ½ poÄet krokÅ¯ a zÃ¡roveÅˆ konvergence (napÅ™. musÃ­ dojÃ­t k poklesu loss funkce v poslednÃ­ch
  deseti iteracÃ­ch) *(0.5 bodu)*

**CÃ­l:** Pochopit a implementovat trÃ©novacÃ­ proces pro uÄenÃ­ lineÃ¡rnÃ­ transformace mezi jazyky.

## PÅ™eklad a vyhodnocenÃ­ â€“ 2 body

**VstupnÃ­ otÃ¡zka:** Jak bychom museli zmÄ›nit matice X a Y, pokud bychom pouÅ¾ili matici W mÃ­sto $W^\top$?

**Ãškol:**

- Na zÃ¡kladÄ› nauÄenÃ© matice **W** implementujte funkci, kterÃ¡ pÅ™eloÅ¾Ã­ zadanÃ© slovo â€“ vÃ½stupem bude 5 nejpodobnÄ›jÅ¡Ã­ch
  slov v cÃ­lovÃ©m jazyce dle kosinovÃ© podobnosti *(1 bod)*
- Otestujte pÅ™esnost na testovacÃ­ch slovech a spoÄÃ­tejte pÅ™esnost pÅ™ekladu (accuracy - top 1, pÅ™Ã­padnÄ› accuracy - top 5)
  *(1 bod)*

**CÃ­l:** OvÄ›Å™it, Å¾e nauÄenÃ¡ transformace umoÅ¾Åˆuje smysluplnÃ½ pÅ™eklad mezi jazyky.

## DoporuÄenÃ© zdroje

- [Prezentace - pÅ™eklady](https://homel.vsb.cz/~vas218/docs/matd/translation.pdf)
- [FastText â€“ pÅ™edtrÃ©novanÃ© modely](https://fasttext.cc/docs/en/crawl-vectors.html)
- [MUSE â€“ dvojjazyÄnÃ© slovnÃ­ky](https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries)

# CviÄenÃ­ 8: CBOW model pro ÄeÅ¡tinu â€“ 20 nebo 30 bodÅ¯ (cviÄenÃ­ a projekt)

V tomto cviÄenÃ­ si vytvoÅ™Ã­te vlastnÃ­ model typu Continuous Bag of Words (CBOW) pro ÄeÅ¡tinu. CÃ­lem je nauÄit model
pÅ™edpovÃ­dat slovo na zÃ¡kladÄ› jeho kontextu a tÃ­m zÃ­skat kvalitnÃ­ distribuovanÃ© vektorovÃ© reprezentace slov (embeddingy).
Ãšloha mÃ¡ dvÄ› varianty: pouÅ¾itÃ­ existujÃ­cÃ­ch knihoven (20 bodÅ¯) nebo plnÃ¡ implementace od nuly (30 bodÅ¯).

## PÅ™Ã­prava a zpracovÃ¡nÃ­ dat â€“ 5 bodÅ¯

**VstupnÃ­ otÃ¡zka:** Co je CBOW model a jak funguje?

**Ãškol:**

- StÃ¡hnÄ›te nebo pouÅ¾ijte pÅ™ipravenÃ½ ÄeskÃ½ korpus (napÅ™. z hugginface.co nebo vÃ½Å™ez Wikipedie).
- ProveÄte tokenizaci a vytvoÅ™te slovnÃ­k (napÅ™. 10 000 nejÄastÄ›jÅ¡Ã­ch slov).
- VytvoÅ™te trÃ©novacÃ­ dvojice: kontextovÃ¡ slova (napÅ™. 2 vlevo + 2 vpravo) â†’ cÃ­lovÃ© slovo.
- Slova mimo slovnÃ­k oznaÄte jako <UNK>.

**CÃ­l:** PÅ™ipravit data pro trÃ©nink CBOW modelu ve formÃ¡tu vhodnÃ©m pro uÄenÃ­.

## TrÃ©novÃ¡nÃ­ modelu â€“ 10 bodÅ¯ (varianta A) nebo 20 bodÅ¯ (varianta B)

**VstupnÃ­ otÃ¡zka:** Co se uÄÃ­ CBOW model a jak se optimalizujÃ­ jeho vÃ¡hy?

**Varianta A â€“ pouÅ¾itÃ­ knihovny (max. 20 bodÅ¯ celkem):**

- VyuÅ¾ijte PyTorch, Keras, TensorFlow nebo jinou knihovnu s Embedding vrstvou.
- Implementujte architekturu CBOW a trÃ©nujte model pomocÃ­ CrossEntropyLoss a optimalizÃ¡toru (SGD/Adam).

**Varianta B â€“ plnÃ¡ implementace (max. 30 bodÅ¯ celkem):**

- Inicializujte matice E, W, b pomocÃ­ napÅ™. normÃ¡lnÃ­ho rozdÄ›lenÃ­.
- VypoÄÃ­tejte prÅ¯mÄ›r embeddingÅ¯ pro kontext.
- Implementujte softmax, ztrÃ¡tu (kÅ™Ã­Å¾ovÃ¡ entropie), vÃ½poÄet gradientÅ¯ a SGD update ruÄnÄ› (napÅ™. v NumPy).
- ProveÄte trÃ©nink pÅ™es vÃ­ce epoch s vÃ½pisem prÅ¯bÄ›hu lossu.

**CÃ­l:** PorozumÄ›t principu CBOW a nauÄit se jej implementovat a trÃ©novat.

## VyhodnocenÃ­ modelu â€“ 5 bodÅ¯

**Ãškol:**

- Pro vybranÃ¡ slova (napÅ™. â€pes", â€Å¡kola", â€krÃ¡snÃ½") najdÄ›te 5 nejbliÅ¾Å¡Ã­ch sousedÅ¯ podle kosinovÃ© podobnosti.
- Vizualizujte embedding prostor pomocÃ­ t-SNE nebo PCA.
- ZhodnoÅ¥te, zda embeddingy zachycujÃ­ vÃ½znamovou podobnost.

**CÃ­l:** OvÄ›Å™it kvalitu nauÄenÃ½ch embeddingÅ¯ a jejich interpretaci.

## DoporuÄenÃ© zdroje:

- [Jurafsky & Martin â€“ Speech and Language Processing (kap. 6)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)
- [Datasety](https://huggingface.co/datasets?task_categories=task_categories:text-classification&language=language:cs&p=1&sort=trending)
- [Prezentace â€“ CBOW model (ke cviÄenÃ­)](https://homel.vsb.cz/~vas218/docs/matd/cbow-prezentace.pdf)

# CviÄenÃ­ 9: Transformer pro sumarizaci dialogÅ¯ â€“ 21 bodÅ¯ (cviÄenÃ­ a projekt)

V tomto cviÄenÃ­ si vyzkouÅ¡Ã­te implementaci modelu typu encoder-decoder zaloÅ¾enÃ©ho na architektuÅ™e Transformer pro Ãºlohu
sumarizace krÃ¡tkÃ½ch dialogÅ¯ ze Samsum datasetu. CÃ­lem je vytvoÅ™it systÃ©m, kterÃ½ dokÃ¡Å¾e automaticky vygenerovat souhrn
danÃ©ho dialogu.

## PÅ™Ã­prava a zpracovÃ¡nÃ­ dat â€“ 2 body

**OdpovÄ›zte:** JakÃ¡ je struktura Samsum datasetu a proÄ je vhodnÃ½ pro Ãºlohu sumarizace?

**Ãškol:**

- StÃ¡hnÄ›te Samsum dataset (napÅ™. pomocÃ­ knihovny datasets z Hugging
  Face). [Odkaz na HuggingFace](https://huggingface.co/datasets/Samsung/samsum)
- RozdÄ›lte data na trÃ©novacÃ­ (pÅ™Ã­p. takÃ© validaÄnÃ­) a testovacÃ­ mnoÅ¾inu.

**CÃ­l:** PÅ™ipravit trÃ©novacÃ­ a testovacÃ­ data vhodnÃ¡ pro modelovÃ¡nÃ­ sumarizace.

## PÅ™edzpracovÃ¡nÃ­ a tokenizace â€“ 2 body

**OdpovÄ›zte:** Jak funguje tokenizace a proÄ je dÅ¯leÅ¾itÃ¡ pÅ™i prÃ¡ci s modely pro textovÃ¡ data?

**Ãškol:**

- Vyberte libovolnÃ½ pÅ™edtrÃ©novanÃ½ tokenizer.
- Tokenizujte vstupnÃ­ texty (dialogy) i vÃ½stupy (shrnutÃ­).

**CÃ­l:** PÅ™evÃ©st textovÃ¡ data do tokenizovanÃ©ho formÃ¡tu vhodnÃ©ho pro trÃ©nink modelu.

## Implementace Transformer modelu â€“ 6 bodÅ¯

**OdpovÄ›zte:** Jak funguje encoder-decoder architektura zaloÅ¾enÃ¡ na Transformeru?

**OdpovÄ›zte:** ProÄ potÅ™ebujeme padding v encoderu a decoderu?

**Ãškol:**

- Naimplementujte encoder-decoder model pro generovÃ¡nÃ­ shrnutÃ­ pomocÃ­ Transformeru.
- MÅ¯Å¾ete vyuÅ¾Ã­t:
    - nn.Transformer z PyTorch,
    - nebo TransformerEncoder a TransformerDecoder vrstvy v Kerasu.
- PÅ™idejte token embedding a poziÄnÃ­ encoding pro vstupnÃ­ i cÃ­lovÃ© sekvence.
- Implementujte sprÃ¡vnÃ© maskovÃ¡nÃ­:
    - Padding mask â€“ ignorovÃ¡nÃ­ padding tokenÅ¯,
    - Causal mask â€“ zÃ¡kaz nahlÃ­Å¾enÃ­ na budoucÃ­ tokeny v dekodÃ©ru.

**CÃ­l:** Postavit funkÄnÃ­ Transformer model pÅ™ipravenÃ½ k trÃ©ninku.

## TrÃ©novÃ¡nÃ­ modelu â€“ 6 body

**OdpovÄ›zte:** Jak se trÃ©nuje sekvenÄnÃ­ model na Ãºlohu sekvence-sekvence?

**Ãškol:**

- Definujte loss funkci (napÅ™Ã­klad CrossEntropyLoss pro predikci tokenÅ¯).
- Vyberte vhodnÃ½ optimalizÃ¡tor (napÅ™Ã­klad Adam/AdamW).
- NatrÃ©nujte model na trÃ©novacÃ­ch datech (validujte bÄ›hem trÃ©novÃ¡nÃ­).

**CÃ­l:** VytrÃ©novat model schopnÃ½ generovat shrnutÃ­.

## Inference â€“ generovÃ¡nÃ­ shrnutÃ­ â€“ 4 body

**OdpovÄ›zte:** JakÃ½m zpÅ¯sobem lze generovat vÃ½stupnÃ­ sekvenci v dekodÃ©rovÃ©m Transformeru?

**NÃ¡povÄ›da:** K Äemu nÃ¡m jsou speciÃ¡lnÃ­ tokeny <SOS> a <EOS>?

**Ãškol:**

- Implementujte funkci, kterÃ¡ na zÃ¡kladÄ› vstupnÃ­ho dialogu vygeneruje shrnutÃ­.
- Nastudujte si a vyuÅ¾ijte greedy decoding nebo beam search.

**CÃ­l:** UmÄ›t z trÃ©novanÃ©ho modelu zÃ­skat vÃ½stupnÃ­ shrnutÃ­.

## VyhodnocenÃ­ modelu â€“ 1 bod

**Ãškol:**

- VyhodnoÅ¥te kvalitu generovanÃ½ch shrnutÃ­ pomocÃ­ metrik ROUGE-1 a ROUGE-2.
- Porovnejte generovanÃ¡ shrnutÃ­ s referenÄnÃ­mi shrnutÃ­mi v testovacÃ­ mnoÅ¾inÄ›.

**CÃ­l:** KvantitativnÄ› ovÄ›Å™it kvalitu modelu pro sumarizaci.

## DoporuÄenÃ© zdroje:

- [Transformer - Jurafsky, Stanford](https://web.stanford.edu/~jurafsky/slp3/10.pdf)

