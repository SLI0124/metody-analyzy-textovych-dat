# CviÄenÃ­ 1: AutomatickÃ© doplÅˆovÃ¡nÃ­ slov (N-gram model)

V tÃ©to Ãºloze byste si mÄ›li vyzkouÅ¡et vytvoÅ™it jednoduchÃ½ model jazyka. Model jazyka je statistickÃ½m modelem, kterÃ½ se
snaÅ¾Ã­ odhadnout pravdÄ›podobnost vÃ½skytu slova na zÃ¡kladÄ› jeho kontextu. V tÃ©to Ãºloze se zamÄ›Å™Ã­me na n-gram model, kterÃ½
je zaloÅ¾en na pravdÄ›podobnosti vÃ½skytu n po sobÄ› jdoucÃ­ch slov. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci
v libovolnÃ©m rozsahu.

## ZÃ¡kladnÃ­ seznÃ¡menÃ­ s n-gramy (jednoduÅ¡Å¡Ã­) - 1 bod

### Ãškol:

- NaÄtÄ›te zvolenÃ½ ÄeskÃ½ textovÃ½ soubor napÅ™. [opus.nlpl.eu](https://opus.nlpl.eu/results/en&cs/corpus-result-table).
- RozdÄ›lte text na tokeny (slova). MÅ¯Å¾ete si vybrat, zda odstranÃ­te interpunkci (pokud ÄÃ¡rky a teÄky v textu zachovÃ¡te,
  pak je berte jako jedno slovo) a zda pÅ™evedete text na malÃ¡ pÃ­smena.
- VytvoÅ™te unigramovÃ½ (jednoslovnÃ½), bigramovÃ½ (dvoslovnÃ½) a trigramovÃ½ (tÅ™Ã­slovnÃ½) model frekvencÃ­ slov.
- Zobrazte nejÄastÄ›jÅ¡Ã­ n-gramy.

### CÃ­l:

NauÄit se zÃ¡kladnÃ­ prÃ¡ci s textem a pochopit, co jsou n-gramy.

## PravdÄ›podobnost n-gramÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Na zÃ¡kladÄ› VaÅ¡ich dat vytvoÅ™te bigramovÃ½ a trigramovÃ½ model pÅ™epoÄÃ­tanÃ½ na pravdÄ›podobnosti.
- VypoÄÃ­tejte pravdÄ›podobnost vÃ½skytu slova nÃ¡sledujÃ­cÃ­ho po urÄitÃ©m n-gramu.
- ZjistÄ›te, co to je Laplaceovo vyhlazovÃ¡nÃ­ a upravte vÃ½poÄet pravdÄ›podobnosti.

### CÃ­l:

SeznÃ¡mit se s pravdÄ›podobnostnÃ­m modelem n-gramÅ¯ a zÃ¡kladnÃ­m vyhlazovÃ¡nÃ­m.

## Predikce slova pomocÃ­ bigramovÃ©ho modelu (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Implementujte jednoduchÃ½ autokomplet, kterÃ½ na zÃ¡kladÄ› zadanÃ©ho slova nabÃ­dne nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo.
- Testujte na rÅ¯znÃ½ch vstupech a porovnejte vÃ½sledky.

### CÃ­l:

Pochopit princip predikce slov a bÃ½t schopen vytvoÅ™it zÃ¡kladnÃ­ model automatickÃ©ho doplÅˆovÃ¡nÃ­.

## VytvoÅ™enÃ­ generÃ¡toru textu (obtÃ­Å¾nÄ›jÅ¡Ã­) - 2 body

### Ãškol:

- PomocÃ­ trigramovÃ©ho modelu vytvoÅ™te jednoduchÃ½ generÃ¡tor textu.
- Model by mÄ›l na zÃ¡kladÄ› vstupnÃ­ho slova vygenerovat dalÅ¡Ã­ slova a vytvoÅ™it nÄ›kolik vÄ›t.

### CÃ­l:

Aplikovat n-gram model pro tvorbu textu.

## Evaluace modelu pomocÃ­ perplexity (nÃ¡roÄnÄ›jÅ¡Ã­) - 2 body

## Ãškol:

- Nastudujte si pojem perplexity.
- Implementujte metodu pro vÃ½poÄet perplexity trÃ©novanÃ©ho n-gram modelu.
- Porovnejte perplexity rÅ¯znÃ½ch n-gram modelÅ¯ (unigramy, bigramy, trigramy).
- ZhodnoÅ¥te kvalitu modelu na testovacÃ­m datasetu.

### CÃ­l:

NauÄit se mÄ›Å™it kvalitu jazykovÃ©ho modelu a pochopit vÃ½znam perplexity.

## KontrolnÃ­ otÃ¡zky k Ãºloze: AutomatickÃ© doplÅˆovÃ¡nÃ­ slov (N-gram model)

- ZÃ¡kladnÃ­ seznÃ¡menÃ­ s n-gramy
    - Co je to n-gram?
    - Jak se poÄÃ­tajÃ­ frekvence unigramÅ¯, bigramÅ¯ a trigramÅ¯?
- PravdÄ›podobnost n-gramÅ¯
    - Jak se poÄÃ­tÃ¡ pravdÄ›podobnost vÃ½skytu slova v rÃ¡mci n-gramovÃ©ho modelu?
    - Co je Laplaceovo vyhlazovÃ¡nÃ­ a proÄ se pouÅ¾Ã­vÃ¡?
- Predikce slova pomocÃ­ bigramovÃ©ho modelu
    - Jak lze pouÅ¾Ã­t bigramovÃ½ model pro automatickÃ© doplÅˆovÃ¡nÃ­ slov?
    - Jak se urÄuje nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo?
- VytvoÅ™enÃ­ generÃ¡toru textu
    - Jak lze trigramovÃ½ model vyuÅ¾Ã­t pro generovÃ¡nÃ­ textu?
    - Jak lze vylepÅ¡it kvalitu generovanÃ©ho textu?
- Evaluace modelu pomocÃ­ perplexity
    - Co je to perplexity a jak se vypoÄÃ­tÃ¡vÃ¡?
    - JakÃ¡ je interpretace hodnoty perplexity pro jazykovÃ½ model?
    - Jak se perplexity mÄ›nÃ­ s rostoucÃ­ velikostÃ­ n-gramu?

# CviÄenÃ­ 2: PÅ™Ã­mÃ© vyhledÃ¡vÃ¡nÃ­ v textovÃ½ch datech - 7 bodÅ¯

V tÃ©to Ãºloze budete analyzovat rÅ¯znÃ© algoritmy pro vyhledÃ¡vÃ¡nÃ­ vzorÅ¯ v textu. ZamÄ›Å™Ã­te se na porovnÃ¡nÃ­ tÅ™Ã­ algoritmÅ¯:
hrubÃ© sÃ­ly, Knuth-Morris-Pratt (KMP) a Boyer-Moore-Horspool (BMH). CÃ­lem je pochopit, kdy je kterÃ½ algoritmus vÃ½hodnÄ›jÅ¡Ã­
a jak se chovajÃ­ pÅ™i rÅ¯znÃ½ch typech textÅ¯ a vzorÅ¯. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m
rozsahu.

## PÅ™Ã­prava implementacÃ­ (jednoduÅ¡Å¡Ã­) - 2 body

### Ãškol:

- PÅ™ipravte implementace tÅ™Ã­ algoritmÅ¯: hrubÃ¡ sÃ­la, KMP a BMH (mÅ¯Å¾ete vyuÅ¾Ã­t AI).
- Upravte algoritmy tak, aby vracely nejen nalezenÃ© pozice vzoru, ale i poÄet porovnÃ¡nÃ­ znakÅ¯.

### CÃ­l:

ZÃ­skat implementace algoritmÅ¯ a zajistit, aby poskytovaly statistiky o porovnÃ¡nÃ­ znakÅ¯.

## TestovÃ¡nÃ­ na rÅ¯znÃ½ch datech (stÅ™ednÃ­) - 2 body

### Ãškol:

Otestujte implementace na tÅ™ech rÅ¯znÃ½ch typech textÅ¯:

- KrÃ¡tkÃ½ text (~100 znakÅ¯)
- DlouhÃ½ text (~1000 znakÅ¯)
- NÃ¡hodnÄ› generovanÃ½ text z malÃ© abecedy (cca do ÄtyÅ™ znakÅ¯, napÅ™. sekvence DNA â€AGCTAGCTâ€¦â€œ)

Pro kaÅ¾dÃ½ typ textu proveÄte testy s alespoÅˆ tÅ™emi rÅ¯znÃ½mi vzory.

### CÃ­l:

OvÄ›Å™it, jak algoritmy fungujÃ­ na rÅ¯znÃ½ch datech.

## PorovnÃ¡nÃ­ poÄtu porovnÃ¡nÃ­ znakÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- Zaznamenejte poÄet porovnÃ¡nÃ­ znakÅ¯ pro kaÅ¾dÃ½ algoritmus a kaÅ¾dou testovacÃ­ sadu.
- VytvoÅ™te tabulku s vÃ½sledky.

### CÃ­l:

Kvantifikovat efektivitu algoritmÅ¯.

## Vizualizace vÃ½konu algoritmÅ¯ (stÅ™ednÃ­) - 1 bod

### Ãškol:

- VytvoÅ™te graf, kterÃ½ ukÃ¡Å¾e efektivitu algoritmÅ¯ v zÃ¡vislosti na dÃ©lce textu a vzoru.

### CÃ­l:

Graficky zobrazit vÃ½konnost algoritmÅ¯.

## AnalÃ½za a rozhodovÃ¡nÃ­ o vhodnosti algoritmÅ¯ (nÃ¡roÄnÄ›jÅ¡Ã­) - 1 bod

### Ãškol:

OdpovÄ›zte na nÃ¡sledujÃ­cÃ­ otÃ¡zky:

- Kdy se KMP chovÃ¡ lÃ©pe neÅ¾ BMH?
- Kdy je BMH rychlejÅ¡Ã­ neÅ¾ Brute Force?
- Kdy je KMP nevÃ½hodnÃ© pouÅ¾Ã­vat?
- Jak algoritmy fungujÃ­ na textech s opakujÃ­cÃ­mi se vzory?

### CÃ­l:

Pochopit silnÃ© a slabÃ© strÃ¡nky jednotlivÃ½ch algoritmÅ¯.

## ğŸ¯ BonusovÃ¡ Ãºloha (+2 body navÃ­c)

### Ãškol:

NavrhnÄ›te hybridnÃ­ pÅ™Ã­stup:

- VytvoÅ™te heuristiku, kterÃ¡ na zÃ¡kladÄ› dÃ©lky a vlastnostÃ­ vzoru a textu vybere nejvhodnÄ›jÅ¡Ã­ algoritmus.
- Porovnejte vÃ½konnost tÃ©to strategie oproti jednotlivÃ½m algoritmÅ¯m.

# CviÄenÃ­ 3: AutomatickÃ¡ oprava slov a vyhledÃ¡vÃ¡nÃ­ s chybou - 7 bodÅ¯

V tÃ©to Ãºloze budete implementovat algoritmus pro automatickou opravu slov a analyzovat efektivitu rÅ¯znÃ½ch pÅ™Ã­stupÅ¯ k
vyhledÃ¡vÃ¡nÃ­ slov s chybou. ZÃ¡kladnÃ­ inspiracÃ­ pro implementaci je znÃ¡mÃ½ algoritmus Petera Norwiga. VaÅ¡Ã­m cÃ­lem bude
implementovat vÃ½poÄet editaÄnÃ­ vzdÃ¡lenosti a nÃ¡slednÄ› vytvoÅ™it systÃ©m pro automatickou opravu slov na zÃ¡kladÄ›
pravdÄ›podobnosti vÃ½skytu slov ve slovnÃ­ku. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m
rozsahu.

## VÃ½poÄet editaÄnÃ­ vzdÃ¡lenosti (jednoduÅ¡Å¡Ã­) - 3 body

### Ãškol:

- Implementujte algoritmus pro vÃ½poÄet Levenshteinovy vzdÃ¡lenosti mezi dvÄ›ma slovy.
- Otestujte vaÅ¡i implementaci na vlastnÃ­ch datech â€“ vyberte nÄ›kolik dvojic slov a ovÄ›Å™te, zda vzdÃ¡lenost odpovÃ­dÃ¡
  oÄekÃ¡vÃ¡nÃ­.

### CÃ­l:

PorozumÄ›t principu editaÄnÃ­ vzdÃ¡lenosti a zajistit sprÃ¡vnou implementaci.

## Implementace automatickÃ© opravy slov (stÅ™ednÃ­) - 4 body

### PÅ™Ã­prava slovnÃ­ku - 1 bod

- VytvoÅ™te slovnÃ­k slov na zÃ¡kladÄ› zvolenÃ©ho datasetu.
- UloÅ¾te frekvenci jednotlivÃ½ch slov, aby bylo moÅ¾nÃ© urÄit jejich pravdÄ›podobnost vÃ½skytu.

### GenerovÃ¡nÃ­ variant slov - 1 bod

- Pro vstupnÃ­ slovo vygenerujte vÅ¡echny moÅ¾nÃ© varianty slov s editaÄnÃ­ vzdÃ¡lenostÃ­ maximÃ¡lnÄ› 2.
- UvaÅ¾ujte operace vloÅ¾enÃ­, smazÃ¡nÃ­, nahrazenÃ­, prohozenÃ­ sousedÅ¯.
- ZjistÄ›te poÄet variant.

### VÃ½bÄ›r nejpravdÄ›podobnÄ›jÅ¡Ã­ho slova - 1 bod

- Z vygenerovanÃ½ch variant vyberte nejpravdÄ›podobnÄ›jÅ¡Ã­ slovo podle jeho Äetnosti ve slovnÃ­ku.
- Opravte nÃ¡sledujÃ­cÃ­ vÄ›tu:  
  _Dneska si dÃ¡m obÄ›Å¥ v restauarci a pak pÅ¯jdu zpÄ›Å¥ domÅ¯, kde se podÃ­vÃ¡m na televezÃ­._

### AlternativnÃ­ pÅ™Ã­stup a porovnÃ¡nÃ­ efektivity - 1 bod

- MÃ­sto generovÃ¡nÃ­ variant vypoÄÃ­tejte editaÄnÃ­ vzdÃ¡lenost ke vÅ¡em slovÅ¯m ve slovnÃ­ku a vyberte nejbliÅ¾Å¡Ã­ kandidÃ¡ty.
- Porovnejte vÃ½poÄetnÃ­ sloÅ¾itost a kvalitu vÃ½sledkÅ¯ obou pÅ™Ã­stupÅ¯.
- Pro dÃ©lku _n_ nÄ›jakÃ©ho slova urÄete poÄet vygenerovanÃ½ch variant.

## ğŸ¯ BonusovÃ¡ Ãºloha (+2 body navÃ­c)

- VylepÅ¡enÃ­ vÃ½poÄtu pravdÄ›podobnosti pomocÃ­ n-gram modelu:
    - NavrhnÄ›te a implementujte vylepÅ¡enÃ½ systÃ©m, kterÃ½ vyuÅ¾Ã­vÃ¡ n-gramy a podmÃ­nÄ›nÃ© pravdÄ›podobnosti pro urÄenÃ­
      nejpravdÄ›podobnÄ›jÅ¡Ã­ho opravenÃ©ho slova.
    - Porovnejte vÃ½sledky s pÅ¯vodnÃ­m pÅ™Ã­stupem a analyzujte zlepÅ¡enÃ­.

# CviÄenÃ­ 4: Boolean Information Retrieval â€“ InvertovanÃ½ index a dotazy â€“ 7 bodÅ¯

V tÃ©to Ãºloze si vyzkouÅ¡Ã­te zÃ¡kladnÃ­ principy booleovskÃ©ho vyhledÃ¡vÃ¡nÃ­ v textovÃ½ch datech. VytvoÅ™Ã­te invertovanÃ½ index s
normalizacÃ­ tokenÅ¯, naparsujete a vyhodnotÃ­te dotazy se zÃ¡vorkami a rÅ¯znÃ½mi logickÃ½mi operÃ¡tory. NÃ¡slednÄ› rozÅ¡Ã­Å™Ã­te svÅ¯j
systÃ©m o kompaktnÃ­ reprezentaci indexu a analyzujete jeho efektivitu. Ãšloha je urÄena pro hlubÅ¡Ã­ pochopenÃ­ principÅ¯
klasickÃ©ho IR modelu. PÅ™i Å™eÅ¡enÃ­ tÃ©to Ãºlohy mÅ¯Å¾ete pouÅ¾Ã­vat umÄ›lou inteligenci v libovolnÃ©m rozsahu.

### Ke studiu

- [Introduction to Information
  Retrieval â€“ Chapter 1, pages 7-9.](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)

## Ãškoly

### InvertovanÃ½ index s normalizacÃ­ â€“ 1 bod

**Ãškol:**

- VytvoÅ™te textovÃ½ korpus (alespoÅˆ 50 dokumentÅ¯) a vytvoÅ™te invertovanÃ½ index.
- PÅ™i tvorbÄ› indexu odstraÅˆte stop slova.
- Index by mÄ›l uchovÃ¡vat takÃ© Äetnost vÃ½skytu kaÅ¾dÃ©ho slova v jednotlivÃ½ch dokumentech.

**CÃ­l:**  
SeznÃ¡mit se s reprezentacÃ­ textovÃ©ho korpusu pomocÃ­ invertovanÃ©ho indexu a pÅ™edzpracovÃ¡nÃ­m textu.

### ParsovÃ¡nÃ­ a vyhodnocenÃ­ sloÅ¾itÃ½ch boolean dotazÅ¯ â€“ 2 body

**Ãškol:**

- Implementujte zpracovÃ¡nÃ­ boolean dotazÅ¯ vÄetnÄ› operÃ¡torÅ¯ `AND`, `OR`, `NOT`.
- VyhodnocenÃ­ proveÄte jako mnoÅ¾inovÃ© operace nad invertovanÃ½m indexem.

**CÃ­l:**  
NauÄit se sprÃ¡vnÄ› parsovat sloÅ¾itÄ›jÅ¡Ã­ logickÃ© dotazy a efektivnÄ› je vyhodnocovat.

### Efektivita a velikost indexu â€“ 1 bod

**Ãškol:**

- Analyzujte velikost vytvoÅ™enÃ©ho indexu: kolik obsahuje tokenÅ¯, prÅ¯mÄ›rnÃ¡ dÃ©lka seznamÅ¯, celkovÃ½ poÄet zÃ¡znamÅ¯.

**CÃ­l:**  
UvÄ›domit si nÃ¡roky invertovanÃ©ho indexu.

### RozhranÃ­ pro dotazovÃ¡nÃ­ a srovnÃ¡nÃ­ dotazÅ¯ â€“ 1 bod

**Ãškol:**

- VytvoÅ™te jednoduchÃ© rozhranÃ­ (napÅ™. konzolovÃ© nebo skriptovÃ©), kterÃ© umoÅ¾nÃ­ zadÃ¡nÃ­ libovolnÃ©ho dotazu a zobrazenÃ­
  vÃ½sledkÅ¯.
- Zobrazte alespoÅˆ ID dokumentÅ¯ nebo prvnÃ­ vÄ›tu z kaÅ¾dÃ©ho nalezenÃ©ho dokumentu.

**CÃ­l:**  
Usnadnit prÃ¡ci s vyhledÃ¡vaÄem a demonstrovat dopad rÅ¯znÃ½ch dotazÅ¯ na vÃ½stup.

### RozÅ¡Ã­Å™enÃ½ boolean model s vÃ¡hovÃ¡nÃ­m â€“ 2 body

**Ãškol:**

- NavrhnÄ›te a implementujte jednoduchÃ½ rozÅ¡Ã­Å™enÃ½ boolean model, kterÃ½ umoÅ¾Åˆuje pÅ™iÅ™azenÃ­ skÃ³re dokumentÅ¯m.
- Na vstupu je dotaz, na vÃ½stupu seÅ™azenÃ½ seznam dokumentÅ¯ podle skÃ³re relevance.
- Porovnejte kvalitu vÃ½sledkÅ¯ oproti ÄistÃ©mu boolean pÅ™Ã­stupu.

**CÃ­l:**  
SeznÃ¡mit se s principem vÃ¡Å¾enÃ©ho boolean modelu a Ãºvodem do relevance ranking.
